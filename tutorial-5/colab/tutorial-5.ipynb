{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial-5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIpzLkPhgH8w3yvyZVM89/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Offliners/RobotLab-MLDL-Training-2022/blob/main/tutorial-5/colab/tutorial-5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 5 - Super Mario"
      ],
      "metadata": {
        "id": "8Udlvre62vWV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWqvLiaf2ppb"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary packages"
      ],
      "metadata": {
        "id": "PLXEddVF3QRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.21.0\n",
        "!pip install gym-super-mario-bros==7.3.0\n",
        "!pip install stable-baselines3==1.6.0"
      ],
      "metadata": {
        "id": "V9vigs003Q7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages"
      ],
      "metadata": {
        "id": "FHsvF37U3Cv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib\n",
        "import gym_super_mario_bros\n",
        "import gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym.wrappers import GrayScaleObservation\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from IPython.display import display, HTML"
      ],
      "metadata": {
        "id": "KOBIgYZP3Eph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runtime Arguments"
      ],
      "metadata": {
        "id": "GeBytHUU35L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    def __getitem__(self, key):\n",
        "        return getattr(self, key)\n",
        "\n",
        "    def __setitem__(self, key, val):\n",
        "        setattr(self, key, val)\n",
        "\n",
        "    #@markdown Random seed\n",
        "    seed = 3366 #@param\n",
        "\n",
        "    #@markdown Super Mario Environment Setting\n",
        "    world = 1 #@param\n",
        "    stage = 1 #@param\n",
        "    version = 0 #@param\n",
        "    action_type = 'simple' #@param\n",
        "    num_skip_frame = 4 #@param\n",
        "    downsample_rate = 3 #@param\n",
        "    num_stack_frame = 4 #@param\n",
        "\n",
        "    #@markdown Hyperparameter\n",
        "    total_timestep = 400000 #@param\n",
        "    max_step = 1000 #@param\n",
        "    step = 512 #@param\n",
        "    episode = 20 #@param\n",
        "    lr = 5e-5 #@param\n",
        "    epoch = 10 #@param\n",
        "    batchsize = 128 #@param\n",
        "    gamma = 0.9 #@param\n",
        "    check_freq = 10000 #@param\n",
        "\n",
        "    #@markdown Workspace setting\n",
        "    save_model_dir = './checkpoints/model' #@param\n",
        "    tensorboard = './checkpoints/tensorboard' #@param\n",
        "    output_video = './video' #@param\n",
        "\n",
        "\n",
        "args = Args()"
      ],
      "metadata": {
        "id": "66SHZGIT36kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Workspace"
      ],
      "metadata": {
        "id": "JTygPhRf5M2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./checkpoints', exist_ok=True)\n",
        "os.makedirs('./checkpoints/model', exist_ok=True)\n",
        "os.makedirs(args.tensorboard, exist_ok=True)\n",
        "os.makedirs(args.output_video, exist_ok=True)"
      ],
      "metadata": {
        "id": "MVT4P3aP5Nn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Random Seed"
      ],
      "metadata": {
        "id": "VBfBxGeL5cAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def same_seed(seed): \n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "QXUrnuE75cwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create environment"
      ],
      "metadata": {
        "id": "dSWohho75nJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if done:\n",
        "                break\n",
        "        return obs, reward, done, info\n",
        "\n",
        "\n",
        "class Downsample(gym.ObservationWrapper):\n",
        "    def __init__(self, env, ratio):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        (oldh, oldw, oldc) = env.observation_space.shape\n",
        "        newshape = (oldh//ratio, oldw//ratio, oldc)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
        "            shape=newshape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, frame):\n",
        "        height, width, _ = self.observation_space.shape\n",
        "        frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)\n",
        "        if frame.ndim == 2:\n",
        "            frame = frame[:,:,None]\n",
        "        return frame\n",
        "\n",
        "\n",
        "class CustomRewardEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "    \n",
        "    def step(self, action):\n",
        "        state, reward, done, info = self.env.step(action)\n",
        "        \n",
        "        if done: \n",
        "            reward = 1 if info['flag_get'] else -1\n",
        "        else:\n",
        "            reward /= 30\n",
        "        \n",
        "        return state, reward, done, info\n",
        "\n",
        "\n",
        "env = gym_super_mario_bros.make(f'SuperMarioBros-{args['world']}-{args['stage']}-v{args['version']')\n",
        "\n",
        "if args['action_type'] == \"right\":\n",
        "    actions = RIGHT_ONLY\n",
        "elif args['action_type'] == \"simple\":\n",
        "    actions = SIMPLE_MOVEMENT\n",
        "elif args['action_type'] == 'complex':\n",
        "    actions = COMPLEX_MOVEMENT\n",
        "else:\n",
        "    print('Unknown action type!')\n",
        "\n",
        "env = JoypadSpace(env, actions)\n",
        "env = SkipFrame(env, skip=args['num_skip_frame'])\n",
        "env = CustomRewardEnv(env)\n",
        "env = GrayScaleObservation(env, keep_dim=True)\n",
        "env = Downsample(env, args['downsample_rate'])\n",
        "env = DummyVecEnv([lambda: env])\n",
        "env = VecFrameStack(env, args['num_stack_frame'], channels_order='last')"
      ],
      "metadata": {
        "id": "d-EojGTo5vq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Callback"
      ],
      "metadata": {
        "id": "e9iqDLOQ6Asn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainCallback(BaseCallback):\n",
        "    def __init__(self, args, env, model, verbose=1):\n",
        "        super(TrainCallback, self).__init__(verbose)\n",
        "        self.args = args\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "        self.writer = SummaryWriter(args['tensorboard'])\n",
        "\n",
        "    def _on_step(self):\n",
        "        n_episodes = self.args['episode']\n",
        "        if self.n_calls % self.args['check_freq'] == 0:\n",
        "            model_path = os.path.join(self.args['save_model_dir'], 'mario_world_{}_{}.pth'.format(self['args.world'], self['args.stage']))\n",
        "            self.model.save(model_path)\n",
        "\n",
        "            total_reward = [0] * n_episodes\n",
        "            total_time = [0] * n_episodes\n",
        "            best_reward = 0\n",
        "            for i in range(n_episodes):\n",
        "                state = self.env.reset()\n",
        "                done = False\n",
        "                total_reward[i] = 0\n",
        "                total_time[i] = 0\n",
        "                while not done and total_time[i] < self.args['max_step']:\n",
        "                    action, _ = self.model.predict(state)\n",
        "                    state, reward, done, info = self.env.step(action)\n",
        "                    total_reward[i] += reward[0]\n",
        "                    total_time[i] += 1\n",
        "\n",
        "                if total_reward[i] > best_reward:\n",
        "                    best_reward = total_reward[i]\n",
        "\n",
        "                state = self.env.reset()\n",
        "\n",
        "            reward_avg = round(sum(total_reward) / n_episodes, 3)\n",
        "            best_reward = round(best_reward, 3)\n",
        "            print(f'[ Train | {self.n_calls}/{self.args['total_timestep']} ] average reward = {reward_avg}, best reward = {best_reward}')\n",
        "\n",
        "            self.writer.add_scalars('Reward', {'average reward' : reward_avg, 'best reward' : best_reward}, self.n_calls)\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "callback = TrainCallback(args, env, model)"
      ],
      "metadata": {
        "id": "hH7GTwrX6CKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "1PjpDxgT6VmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PPO('CnnPolicy', env, verbose=0, tensorboard_log=args['tensorboard'], learning_rate=args['lr'], n_steps=args['step'],\n",
        "        batch_size=args['batchsize'], n_epochs=args['epoch'], gamma=args['gamma'])"
      ],
      "metadata": {
        "id": "CiwfgMGr6WUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorboard"
      ],
      "metadata": {
        "id": "6CgLpILU6kZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=./checkpoints/tensorboard/"
      ],
      "metadata": {
        "id": "v1ft1Crb6lKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start training!"
      ],
      "metadata": {
        "id": "HM2PSbt36njz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.learn(total_timesteps=args['total_timestep'], callback=callback)"
      ],
      "metadata": {
        "id": "H9cR-Ml76oYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start testing!"
      ],
      "metadata": {
        "id": "e47ozC-07Msh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_episodes = args['episode']\n",
        "total_reward = [0] * n_episodes\n",
        "total_action = [0] * n_episodes\n",
        "best_reward = 0\n",
        "flags = 0\n",
        "frames_best = []\n",
        "for i in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward[i] = 0\n",
        "    total_action[i] = 0\n",
        "    frames = []\n",
        "    flag = 0\n",
        "    while not done and total_action[i] < args['max_step']:\n",
        "        action, _ = model.predict(state)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        total_reward[i] += reward[0]\n",
        "        total_action[i] += 1\n",
        "        frames.append(copy.deepcopy(env.render(mode='rgb_array')))\n",
        "\n",
        "        if info[0][\"flag_get\"]:\n",
        "            flag = 1\n",
        "            flags += 1\n",
        "            break \n",
        "\n",
        "    if total_reward[i] > best_reward:\n",
        "        best_reward = total_reward[i]\n",
        "        frames_best = copy.deepcopy(frames)\n",
        "\n",
        "    if flag:\n",
        "        print(f'[ Test | {i + 1}/{n_episodes} ] reward = {round(total_reward[i], 3)}, action step = {total_action[i]}, World {args['world']}-{args['stage']} completed!')\n",
        "    else:\n",
        "        print(f'[ Test | {i + 1}/{n_episodes} ] reward = {round(total_reward[i], 3)}, action step = {total_action[i]}')\n",
        "\n",
        "avg_action_step = round(sum(total_action) / n_episodes, 3)\n",
        "avg_reward = round(sum(total_reward) / n_episodes, 3)\n",
        "print(f'average reward = {avg_reward}, average action step = {avg_action_step}, best_reward = {round(best_reward, 3)}')\n",
        "print(f'Complete rate : [{flags}/{n_episodes}]')"
      ],
      "metadata": {
        "id": "172Xijcy7Opr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gameplay display"
      ],
      "metadata": {
        "id": "Zx5bCTC37Jem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frames_new = np.array(frames_best)\n",
        "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames_new[0])\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0, h_pad=0, w_pad=0)\n",
        "animate = lambda i: patch.set_data(frames_new[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames_new), interval=50)\n",
        "display(HTML(ani.to_jshtml()))\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "ktyId6lj7L5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download gameplay video"
      ],
      "metadata": {
        "id": "e44cJF1E7nzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FFwriter = animation.FFMpegWriter(fps=20, extra_args=['-vcodec', 'mpeg4'])\n",
        "ani.save(os.path.join(args['output_video'], f'video_world_{args['world']}_{args['stage']}.mp4'), writer=FFwriter)"
      ],
      "metadata": {
        "id": "P7HGHozc7r2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(os.path.join(args['output_video'], f'video_world_{args['world']}_{args['stage']}.mp4'))"
      ],
      "metadata": {
        "id": "OiWlPkAH750z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}